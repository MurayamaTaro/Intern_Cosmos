nohup: ignoring input

=== START E1_baseline_r16_b32_ep3 (attempt 1) ===
Command: python my_scripts/posttrain_single.py --dataset_path /workspace/dataset_vript/gaming --dataset_name vript_gaming --lora_rank 16 --max_iter 1478 --batch_size_per_gpu 1 --learning_rate 1e-4 --scale 2.0 --grad_accum_iter 4 --seed 0
Log file: runs_cosmos_lora/20250822_142138/logs/E1_baseline_r16_b32_ep3.log
Generated Run Name: vript_gaming_r16_iter1478_bs1_accum4_scale2.0_lr1e-04_seed0
Starting from the base model: /workspace/checkpoints/Cosmos-Predict1-7B-Text2World/model.pt

================================================================================
Starting single-dataset training: text2world_7b_lora_my/vript_gaming_r16_iter1478_bs1_accum4_scale2.0_lr1e-04_seed0
Dataset dir: /workspace/dataset_vript/gaming
Log file: /workspace/logs/text2world_7b_lora_my/vript_gaming_r16_iter1478_bs1_accum4_scale2.0_lr1e-04_seed0/stdout.log
Command: torchrun --nproc_per_node=8 -m cosmos_predict1.diffusion.training.train --config=cosmos_predict1/diffusion/training/config/config.py -- experiment=text2world_7b_lora_my job.name=text2world_7b_lora_my/vript_gaming_r16_iter1478_bs1_accum4_scale2.0_lr1e-04_seed0 checkpoint.load_path=/workspace/checkpoints/Cosmos-Predict1-7B-Text2World/model.pt trainer.max_iter=1478 trainer.seed=0 optimizer.lr=0.0001 model.peft_control.rank=16 model.peft_control.scale=2.0 model.latent_shape=[16,16,36,64] dataloader_train.dataset.video_size=[288, 512] dataloader_train.sampler.dataset.video_size=[288, 512] dataloader_val.dataset.video_size=[288, 512] dataloader_val.sampler.dataset.video_size=[288, 512] dataloader_train.dataset.dataset_dir=/workspace/dataset_vript/gaming dataloader_train.sampler.dataset.dataset_dir=/workspace/dataset_vript/gaming dataloader_val.dataset.dataset_dir=/workspace/dataset_vript/gaming dataloader_val.sampler.dataset.dataset_dir=/workspace/dataset_vript/gaming dataloader_train.batch_size=1 dataloader_val.batch_size=1 trainer.grad_accum_iter=4
================================================================================
W0822 14:21:41.046000 1721894 torch/distributed/run.py:793] 
W0822 14:21:41.046000 1721894 torch/distributed/run.py:793] *****************************************
W0822 14:21:41.046000 1721894 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0822 14:21:41.046000 1721894 torch/distributed/run.py:793] *****************************************
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_hdvila
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_hdvila
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_my
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_hdvila
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world_instruction/experiment.py:215:register_experiments] Registering experiment: video2world_instruction_bridge_57frames
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world_action/experiment.py:217:register_experiments] Registering experiment: video2world_action_bridge_2frames
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/world_interpolator/experiment.py:201:register_experiments] Registering experiment: world_interpolator_7b_example_hdvila
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_hdvila
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_hdvila
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_my
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_hdvila
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_hdvila
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_hdvila
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_hdvila
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_hdvila
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world_instruction/experiment.py:215:register_experiments] Registering experiment: video2world_instruction_bridge_57frames
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/video2world_action/experiment.py:217:register_experiments] Registering experiment: video2world_action_bridge_2frames
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_cosmos_nemo_assets
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/world_interpolator/experiment.py:201:register_experiments] Registering experiment: world_interpolator_7b_example_hdvila
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:54|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_my
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_hdvila
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_my
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_hdvila
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world_instruction/experiment.py:215:register_experiments] Registering experiment: video2world_instruction_bridge_57frames
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world_action/experiment.py:217:register_experiments] Registering experiment: video2world_action_bridge_2frames
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world_instruction/experiment.py:215:register_experiments] Registering experiment: video2world_instruction_bridge_57frames
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/world_interpolator/experiment.py:201:register_experiments] Registering experiment: world_interpolator_7b_example_hdvila
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world_action/experiment.py:217:register_experiments] Registering experiment: video2world_action_bridge_2frames
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/world_interpolator/experiment.py:201:register_experiments] Registering experiment: world_interpolator_7b_example_hdvila
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_hdvila
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_hdvila
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_hdvila
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_hdvila
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_my
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_hdvila
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_my
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_hdvila
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world_instruction/experiment.py:215:register_experiments] Registering experiment: video2world_instruction_bridge_57frames
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world_action/experiment.py:217:register_experiments] Registering experiment: video2world_action_bridge_2frames
[08-22 14:21:55|CRITICAL|cosmos_predict1/utils/distributed.py:68:init] [RANK 4] Initialized distributed program with local rank 4 with timeout 1800
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world_instruction/experiment.py:215:register_experiments] Registering experiment: video2world_instruction_bridge_57frames
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/video2world_action/experiment.py:217:register_experiments] Registering experiment: video2world_action_bridge_2frames
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_hdvila
[08-22 14:21:55|INFO|cosmos_predict1/diffusion/training/config/world_interpolator/experiment.py:201:register_experiments] Registering experiment: world_interpolator_7b_example_hdvila
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_hdvila
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/world_interpolator/experiment.py:201:register_experiments] Registering experiment: world_interpolator_7b_example_hdvila
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_cosmos_nemo_assets
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_hdvila
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_hdvila
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_14b_example_cosmos_nemo_assets
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_my
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_hdvila
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/text2world/experiment.py:1200:register_experiments] Registering experiment: text2world_7b_lora_my
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_hdvila
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world_instruction/experiment.py:215:register_experiments] Registering experiment: video2world_instruction_bridge_57frames
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_80gb
[08-22 14:21:56|CRITICAL|cosmos_predict1/utils/distributed.py:68:init] [RANK 6] Initialized distributed program with local rank 6 with timeout 1800
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world_action/experiment.py:217:register_experiments] Registering experiment: video2world_action_bridge_2frames
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_8gpu_40gb
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_example_cosmos_nemo_assets_4gpu_40gb
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/world_interpolator/experiment.py:201:register_experiments] Registering experiment: world_interpolator_7b_example_hdvila
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world/experiment.py:842:register_experiments] Registering experiment: video2world_7b_lora_example_cosmos_nemo_assets
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world_instruction/experiment.py:215:register_experiments] Registering experiment: video2world_instruction_bridge_57frames
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/video2world_action/experiment.py:217:register_experiments] Registering experiment: video2world_action_bridge_2frames
[08-22 14:21:56|INFO|cosmos_predict1/diffusion/training/config/world_interpolator/experiment.py:201:register_experiments] Registering experiment: world_interpolator_7b_example_hdvila
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[08-22 14:21:56|CRITICAL|cosmos_predict1/utils/distributed.py:68:init] [RANK 5] Initialized distributed program with local rank 5 with timeout 1800
[08-22 14:21:56|CRITICAL|cosmos_predict1/utils/distributed.py:68:init] [RANK 3] Initialized distributed program with local rank 3 with timeout 1800
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[DEBUG] LoRA config generated: {'enabled': True, 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 8, 'scale': 1, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
[08-22 14:21:56|CRITICAL|cosmos_predict1/utils/distributed.py:68:init] [RANK 0] Initialized distributed program with local rank 0 with timeout 1800
[08-22 14:21:57|INFO|cosmos_predict1/utils/distributed.py:78:init] Running with 8 GPUs.
[08-22 14:21:57|CRITICAL|cosmos_predict1/utils/distributed.py:68:init] [RANK 1] Initialized distributed program with local rank 1 with timeout 1800
[08-22 14:21:57|CRITICAL|cosmos_predict1/utils/distributed.py:68:init] [RANK 7] Initialized distributed program with local rank 7 with timeout 1800
[08-22 14:21:57|CRITICAL|cosmos_predict1/utils/distributed.py:68:init] [RANK 2] Initialized distributed program with local rank 2 with timeout 1800
Failed to save config to checkpoints/posttraining/diffusion_text2world/text2world_7b_lora_my/vript_gaming_r16_iter1478_bs1_accum4_scale2.0_lr1e-04_seed0/config.pkl: Can't pickle <function get_sampler at 0x7fa918cf3d00>: it's not the same object as cosmos_predict1.diffusion.training.config.text2world.experiment.get_sampler. Trying dill or cloudpickle instead
Config is saved using dill at checkpoints/posttraining/diffusion_text2world/text2world_7b_lora_my/vript_gaming_r16_iter1478_bs1_accum4_scale2.0_lr1e-04_seed0/config.pkl.
Config is saved using omegaconf at checkpoints/posttraining/diffusion_text2world/text2world_7b_lora_my/vript_gaming_r16_iter1478_bs1_accum4_scale2.0_lr1e-04_seed0/config.yaml.
[08-22 14:22:00|INFO|cosmos_predict1/utils/trainer.py:88:__init__] Config:
* model:
   * vae: {'latent_ch': 16, 'is_bf16': True, 'spatial_compression_factor': 8, 'temporal_compression_factor': 8, 'pixel_chunk_duration': 121, 'max_enc_batch_size': 8, 'max_dec_batch_size': 4, 'spatial_resolution': '720', 'name': 'cosmos_diffusion_tokenizer_comp8x8x8', 'enc_fp': 'checkpoints/Cosmos-Tokenize1-CV8x8x8-720p/encoder.jit', 'dec_fp': 'checkpoints/Cosmos-Tokenize1-CV8x8x8-720p/decoder.jit', 'mean_std_fp': 'checkpoints/Cosmos-Tokenize1-CV8x8x8-720p/mean_std.pt', '_target_': <class 'cosmos_predict1.diffusion.training.module.pretrained_vae.VideoJITTokenizer'>}
   * conditioner: {'text': {'obj': {'_target_': <class 'cosmos_predict1.diffusion.conditioner.TextAttr'>}, 'dropout_rate': 0.0, 'input_keys': ['t5_text_embeddings', 't5_text_mask']}, 'fps': {'obj': {'output_key': 'fps', 'dtype': None, '_target_': <class 'cosmos_predict1.diffusion.config.base.conditioner.ReMapkey'>}, 'dropout_rate': 0.0, 'input_key': 'fps'}, 'num_frames': {'obj': {'output_key': 'num_frames', 'dtype': None, '_target_': <class 'cosmos_predict1.diffusion.config.base.conditioner.ReMapkey'>}, 'dropout_rate': 0.0, 'input_key': 'num_frames'}, 'image_size': {'obj': {'output_key': 'image_size', 'dtype': None, '_target_': <class 'cosmos_predict1.diffusion.config.base.conditioner.ReMapkey'>}, 'dropout_rate': 0.0, 'input_key': 'image_size'}, 'padding_mask': {'obj': {'output_key': 'padding_mask', 'dtype': None, '_target_': <class 'cosmos_predict1.diffusion.config.base.conditioner.ReMapkey'>}, 'dropout_rate': 0.0, 'input_key': 'padding_mask'}, '_target_': <class 'cosmos_predict1.diffusion.training.conditioner.VideoConditioner'>}
   * net: {'concat_padding_mask': True, 'block_config': 'FA-CA-MLP', 'model_channels': 4096, 'num_blocks': 28, 'num_heads': 32, 'window_block_indexes': [], 'window_sizes': [], 'spatial_attn_win_size': 1, 'temporal_attn_win_size': 1, 'mlp_ratio': 4.0, 'use_memory_save': False, 'use_checkpoint': False, 'block_x_format': 'THWBD', 'crossattn_emb_channels': 1024, 'use_cross_attn_mask': False, 'pos_emb_cls': 'rope3d', 'pos_emb_learnable': False, 'pos_emb_interpolation': 'crop', 'min_fps': 1, 'max_fps': 30, 'additional_timestamp_channels': None, 'affline_emb_norm': True, 'use_adaln_lora': True, 'adaln_lora_dim': 256, 'layer_mask': None, 'legacy_patch_emb': False, 'rope_h_extrapolation_ratio': 1, 'rope_w_extrapolation_ratio': 1, 'rope_t_extrapolation_ratio': 2, 'extra_per_block_abs_pos_emb': True, 'extra_per_block_abs_pos_emb_type': 'learnable', 'extra_h_extrapolation_ratio': 1.0, 'extra_w_extrapolation_ratio': 1.0, 'extra_t_extrapolation_ratio': 1.0, 'max_img_h': 240, 'max_img_w': 240, 'max_frames': 128, 'in_channels': 16, 'out_channels': 16, 'patch_spatial': 2, 'patch_temporal': 1, '_target_': <class 'cosmos_predict1.diffusion.training.networks.general_dit.GeneralDIT'>}
   * ema: {'enabled': False, 'model': None, 'rate': 0.1, 'num': 3, '_target_': <bound method PowerEMATracker.initialize_multi_rank_ema of <class 'cosmos_predict1.utils.ema.PowerEMATracker'>>}
   * sde: {'p_mean': 0.0, 'p_std': 1.0, 'sigma_max': 80, 'sigma_min': 0.0002, '_target_': <class 'cosmos_predict1.diffusion.training.modules.edm_sde.EDMSDE'>}
   * sigma_data: 0.5
   * camera_sample_weight: {'enabled': False, 'weight': 5.0}
   * aesthetic_finetuning: {'enabled': False}
   * loss_mask_enabled: False
   * loss_masking: None
   * loss_add_logvar: True
   * precision: bfloat16
   * input_data_key: video
   * input_image_key: images_1024
   * loss_reduce: mean
   * loss_scale: 1.0
   * latent_shape: [16, 16, 36, 64]
   * fsdp_enabled: False
   * use_torch_compile: False
   * fsdp:
      * policy: block
      * checkpoint: False
      * min_num_params: 1024
      * sharding_group_size: 8
      * sharding_strategy: full
   * use_dummy_temporal_dim: False
   * adjust_video_noise: False
   * peft_control: {'enabled': True, 'customization_type': 'LoRA', 'rank': 16, 'scale': 2.0, 'edits': [{'blocks': '\\b(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27)\\b', 'customization_type': 'LoRA', 'rank': 0, 'scale': 1.0, 'block_edit': ['FA[to_q, to_v]', 'CA[to_q, to_v]']}]}
   * num_latents_to_drop: 0
* optimizer: {'optim_type': 'fusedadam', 'sharding': True, 'model': None, 'lr': 0.0001, 'weight_decay': 0.1, 'betas': [0.9, 0.99], 'eps': 1e-10, 'master_weights': True, 'capturable': True, '_target_': <function get_base_optimizer at 0x7fa918cf3490>}
* scheduler: {'verbosity_interval': 0, 'warm_up_steps': [300], 'cycle_lengths': [10000000000000], 'f_start': [1e-06], 'f_max': [1.0], 'f_min': [1.0], '_target_': <class 'cosmos_predict1.diffusion.training.functional.lr_scheduler.LambdaLinearScheduler'>}
* dataloader_train: {'batch_size': 1, 'shuffle': None, 'sampler': {'dataset': {'start_frame_interval': 1, 'dataset_dir': '/workspace/dataset_vript/gaming', 'sequence_interval': 1, 'num_frames': 121, 'video_size': [288, 512], '_target_': <class 'cosmos_predict1.diffusion.training.datasets.dataset_video.Dataset'>}, '_target_': <function get_sampler at 0x7fa918cf3d00>}, 'batch_sampler': None, 'num_workers': 8, 'collate_fn': None, 'pin_memory': True, 'drop_last': True, 'timeout': 0, 'worker_init_fn': None, 'multiprocessing_context': None, 'generator': None, 'prefetch_factor': None, 'persistent_workers': False, 'pin_memory_device': '', 'dataset': {'start_frame_interval': 1, 'dataset_dir': '/workspace/dataset_vript/gaming', 'sequence_interval': 1, 'num_frames': 121, 'video_size': [288, 512], '_target_': <class 'cosmos_predict1.diffusion.training.datasets.dataset_video.Dataset'>}, '_target_': <class 'torch.utils.data.dataloader.DataLoader'>}
* dataloader_val: {'batch_size': 1, 'shuffle': None, 'sampler': {'dataset': {'start_frame_interval': 1, 'dataset_dir': '/workspace/dataset_vript/gaming', 'sequence_interval': 1, 'num_frames': 121, 'video_size': [288, 512], '_target_': <class 'cosmos_predict1.diffusion.training.datasets.dataset_video.Dataset'>}, '_target_': <function get_sampler at 0x7fa918cf3d00>}, 'batch_sampler': None, 'num_workers': 8, 'collate_fn': None, 'pin_memory': True, 'drop_last': True, 'timeout': 0, 'worker_init_fn': None, 'multiprocessing_context': None, 'generator': None, 'prefetch_factor': None, 'persistent_workers': False, 'pin_memory_device': '', 'dataset': {'start_frame_interval': 1, 'dataset_dir': '/workspace/dataset_vript/gaming', 'sequence_interval': 1, 'num_frames': 121, 'video_size': [288, 512], '_target_': <class 'cosmos_predict1.diffusion.training.datasets.dataset_video.Dataset'>}, '_target_': <class 'torch.utils.data.dataloader.DataLoader'>}
* job:
   * project: posttraining
   * group: diffusion_text2world
   * name: text2world_7b_lora_my/vript_gaming_r16_iter1478_bs1_accum4_scale2.0_lr1e-04_seed0
* trainer:
   * type: <class 'cosmos_predict1.utils.trainer.Trainer'>
   * callbacks: {'grad_clip': {'clip_norm': 1.0, 'force_finite': True, 'model_key': 'model', 'fsdp_enabled': False, '_target_': <class 'cosmos_predict1.utils.callbacks.grad_clip.GradClip'>}, 'low_prec': {'config': None, 'trainer': None, 'update_iter': 1, '_target_': <class 'cosmos_predict1.diffusion.training.callbacks.low_precision.LowPrecisionCallback'>}, 'iter_speed': {'hit_thres': 0, 'every_n': 10, '_target_': <class 'cosmos_predict1.diffusion.training.callbacks.iter_speed.IterSpeed'>}}
   * distributed_parallelism: ddp
   * ddp:
      * find_unused_parameters: True
      * static_graph: False
      * broadcast_buffers: True
   * cudnn:
      * deterministic: False
      * benchmark: True
   * seed: 0
   * grad_scaler_args: {'enabled': False}
   * max_iter: 1478
   * max_val_iter: None
   * logging_iter: 50
   * run_validation: False
   * validation_iter: 100
   * timeout_period: 999999999
   * memory_format: torch.preserve_format
   * grad_accum_iter: 4
